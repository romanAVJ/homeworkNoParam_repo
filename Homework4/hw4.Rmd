---
title: "Homework4"
output:
  pdf_document: default
  html_notebook: default
---

```{r, include = FALSE}
set.seed(20200420) #created at date
# assigning number of excercises 
(people_3quest <- sample(x = c("Luis","Roman","Sant","Sof"),replace = F, size = 2)) #people with 3 questions, by alphabetic order
# assinging excersises
number_ex <- 1:10 #excercises
cat("\nEjercicios Luis: ")
(ex_luis <- sample(x = number_ex, replace = F, size = 3))
number_ex <- number_ex[! number_ex %in% ex_luis] #removing questions 
cat("\nEjercicios Roman: ")
(ex_roman <- sample(x = number_ex, replace = F, size = 3))
number_ex <- number_ex[! number_ex %in% ex_roman] #removing questions 
cat("\nEjercicios Sant: ")
(ex_sant <- sample(x = number_ex, replace = F, size = 3))
number_ex <- number_ex[! number_ex %in% ex_sant] #removing questions 
cat("\nEjercicios Sof: ")
(ex_sof <- number_ex)
```

```{r setup, include=FALSE, results=FALSE}
#Instalar las librerias necesarias y cargarlas
instalar <- function(paquete) {
    if (!require(paquete,character.only = TRUE,
                 quietly = TRUE, 
                 warn.conflicts = FALSE)){
          install.packages(as.character(paquete),
                           dependecies = TRUE,
                           repos = "http://cran.us.r-project.org")
          library(paquete,
                  character.only = TRUE, 
                  quietly = TRUE,
                  warn.conflicts = FALSE)
    }
}
libs <- c('sm', 'HistData', 'MASS', 'splines','tidyverse') 
lapply(libs, instalar)
```



# Ejercicio 2. Densidad gaussiana.

Se pide $$\mathbb{P}[X \leq 15]$$.
Se hará de la siguiente manera, con el estimador de Kernel Rosenblatt, con $K_0$ un kernel gaussino.

$$\mathbb{P}[X \leq 15] = \int_{- \infty}^{15}\hat{f}_{n}^{K_0}(x) dx =\frac{1}{\sqrt{9}} \int_{- \infty}^{15}\sum_{i = 1}^{9}\varphi(\frac{x_{i} - x}{\sqrt{1/3}})dx = \frac{1}{9}\sum_{i=1}^{9}\Phi(\frac{15 - x_{i}}{\sqrt{1/3}})$$

```{r forma1}
#data
X <- c(12,13,12,15,15,16,19,20,25)
n <- length(X)
p <- 15
hn <- sqrt(3/n)

#estimate P(X < 15)
s <- 0
for (xi in X){s <- s + pnorm((p-xi)/hn)}
s <- 1/n * s

```

Por tanto,  $$\mathbb{P}[X \leq 15]  \approx $$ `r s`

```{r forma2}
#seed
set.seed(42)

#density estimator
#data
data <- c(12,13,12,15,15,16,19,20,25)
n <- length(X)
p <- 15
hn <- sqrt(3/n)

# Kernel
Kernel <- function(x) {mean(dnorm((x - data)/ hn)/hn )}    #kernel Density
kpdf <- function(x) sapply(x, Kernel)   # Elementwise application


# estimate kernel
x <- seq(5, 30, length = 1000)

#plot 
#total prob 
plot(data, kpdf(data), col = 2, ylim = c(0,.25), xlim = c(5,30))

#plot all
lines(x, kpdf(x))

#rug
rug(data)

#individual kernels
i <- 1
eps <- rnorm(n = n, sd = 0.2)
for(xi in data){
  lines(x, dnorm(x, mean = xi + eps[i], sd = hn)*.1, col = 2)
  i <- i + 1
}

#density with R
lines(density(data, kernel = 'gaussian', bw = 'SJ'), xlim = c(5,30), ylim = c(0,.20), col = 3)



```


La probabilidad estimada de manera numérica está dada por:

```{r}
# estimate probability
integrate(kpdf, - Inf, 15)

```

__NOTA__ 

1. A cambios de $h_n$ , las probabilidades cambian.
2. La estimación de la probabilidad numérica coincide con la teórica.



Ejercicio 3
Generar 20 observaciones de una distribución mezcla donde la mitad de las observaciones son normales estándar y la otra mitad son datos normales N (1,0.64). Usar la función density de R para crear un estimador de la densidad.
```{r}
# Primero generamos las observaciones de la distribución mezcla
X <- rnorm(10)
Y <- rnorm(10,1,0.64)
x <- c(X,Y)
# Observamos el resultado usando un histograma
hist(x, probability = T, col = "grey")
```
El histograma nos permite ver de manera rápida la presencia de dos distribuciones.

```{r}
# Ahora estimaremos la KDE usando la función density()
density(x, bw = "nrd")
# Observamos que el ancho de banda óptimo está cercano a 0.5, por lo tanto usaremos esta cantidad para hacer comparaciones entre distintos kernel´s.
hist(x, xlim = c(-2, 3),probability = T, col = "grey", main = "KDE de la distribución mezcal",ylab="prob",xlab="x´s")
lines(density(x,bw = 0.5, kernel = "gaussian"), lwd =3, ) 
lines(density(x,bw = 0.5, kernel = "rectangular"),col = "blue", lwd =3)
lines(density(x,bw = 0.5, kernel = "epanechnikov"),col = "red", lwd =3)
legend("topright", legend = c("normal","rectangular","epanechnikov"), col = c("black","blue","red"), lwd = c(3,3)) 
```

Primero, observamos que la estimación del histograma no es buena. Entre los distintos Kernel´s, los tres son similares. Cómo sabemos que las observacione vienen de una mezcla de densidades normales, el Kernel Gaussiano podría ser el más conveniente.


##Ejercicio 4

```{r}
data(Michelson)
head(Michelson)
```

```{r}
plot(density(Michelson$velocity,bw = 12),col = "red", lwd =1, main = "KDE de velocidad",ylab="prob",xlab="velocidad")
rug(Michelson$velocity) 
```

```{r}
hist(Michelson$velocity, breaks=15, xlim = c(500,1150), ylim = c(0,0.006), col ="pink", prob = T)
lines(density(Michelson$velocity, bw = "nrd", n = 100)) # factor normal (1.06)
bandwidth.nrd(Michelson$velocity) # ancho de banda cuadruplicado.
lines(density(Michelson$velocity, bw = "nrd0", n = 100), lty=3, col = "red", lwd = 3) #factor con (0.9)

```

##Ejercicio 6
```{r}
set.seed(042020)
u <- runif(100)
x <- sort(u)
y <- x^2 + 0.1*rnorm(100)
```

```{r}
splinefun(x, y,  method = "fmm", ties = mean)
```

```{r}
dat <- data.frame(x, y)
FIT <- lm( y ~ bs(x, df = 4 ), data =dat) 
summary(FIT)
MAT <- bs(dat$x, df = 4) 
X <- dat[[1]]
Ypred <- MAT[, 1] * FIT$coefficients[2] +  
  MAT[, 2] * FIT$coefficients[3] +
  MAT[, 3] * FIT$coefficients[4] +
  MAT[, 4] * FIT$coefficients[5] + FIT$coefficients[1]

plot(dat$x, dat$y)
Pts <- seq(1,10, length.out = 100)
lines(Pts, predict(FIT, newdata = list(x = Pts)))
points(X, Ypred, col = "red")
```


# Ejercicio 7

Datos simulados de accidentes de motocicleta.

```{r}
# load data set
df_motor <- read.table(file = 'motor.dat', stringsAsFactors = FALSE, header = TRUE )
df_motor[,c(3,4)] <- df_motor[,c(3,4)] %>% as.factor()

# loess fitness
df_motor %>% 
  ggplot(aes(x = times, y = accel)) +
  geom_point() +
  geom_smooth(method = 'loess', color = 'navy') +
  geom_smooth(method = 'loess', color = 'red', span = 0.1) + 
  geom_smooth(method = 'loess', color = 'green', span = 0.5)  +
  ggtitle(label = 'loess')
  
# spline
x <- df_motor$times
y <- df_motor$accel

sm.regression(x, y, h = 1) + title(main = 'Spline')

```

##Ejercicio 10

Queremos estimar $\sigma^2$ (error variance)

Cargamos los datos
```{r}
data(radioc)
summary(radioc)
head(radioc)
```
¿Qué h elijo?

```{r}
h.10<-h.select(radioc$Rc.age,radioc$Cal.age)
```

Construimos la matriz de pesos $S$
```{r}
s.10<-sm.weight(radioc$Rc.age,radioc$Cal.age,h=h.10)
```

Construimos el estimador del error de la varianza basado en la suma de residuales al cuadrado

```{r}
reg.10<-sm.regression(radioc$Rc.age,radioc$Cal.age,)
```


Construimos el estimador de diferenciade base 
```{r}
(d.10<-sm.sigma(radioc$Rc.age,radioc$Cal.age))
```

Ahora sí graficamos
```{r}
plot(s.10)
abline(v=d.10, col="blue")
```












